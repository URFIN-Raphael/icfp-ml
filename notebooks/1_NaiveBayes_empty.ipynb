{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlelarge/ens-ml4sd/blob/main/notebooks/3_NaiveBayes_empty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f028de5-097f-446b-aaf1-309b714d2cf3",
      "metadata": {
        "id": "7f028de5-097f-446b-aaf1-309b714d2cf3"
      },
      "source": [
        "# Naive Bayes Binary Classifier\n",
        "\n",
        "In this practical, we will build a Naive Bayes classifier from scratch for binary classification on the Iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "871dd655-5ef5-4bda-88ce-b05d7b9cf95f",
      "metadata": {
        "id": "871dd655-5ef5-4bda-88ce-b05d7b9cf95f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import datasets, model_selection\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "069ad8a2-7bdb-48ae-bb3d-822c2819d12f",
      "metadata": {
        "id": "069ad8a2-7bdb-48ae-bb3d-822c2819d12f"
      },
      "source": [
        "## Load and Prepare the Iris Dataset\n",
        "\n",
        "We will use only the first two features (sepal length and width) and convert the problem to binary classification:\n",
        "- Class 0: Iris-Setosa\n",
        "- Class 1: Iris-Versicolour and Iris-Virginica (combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38de19ca-b2d3-4b4f-a1e1-05f23ed6af25",
      "metadata": {
        "id": "38de19ca-b2d3-4b4f-a1e1-05f23ed6af25"
      },
      "outputs": [],
      "source": [
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "data = iris.data[:, :2]  # Only sepal length and width\n",
        "targets = iris.target\n",
        "\n",
        "# Convert to binary classification\n",
        "targets_binary = np.where(targets == 0, 0, 1)\n",
        "\n",
        "# Split into train and test sets\n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
        "    data, targets_binary, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0506bff-2363-4e13-81f6-329830f17a95",
      "metadata": {
        "id": "d0506bff-2363-4e13-81f6-329830f17a95"
      },
      "outputs": [],
      "source": [
        "# Visualize the training data\n",
        "labels = {0: 'Iris-Setosa', 1: 'Others'}\n",
        "colors = ['blue', 'red']\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "for c in [0, 1]:\n",
        "    mask = y_train == c\n",
        "    plt.scatter(x_train[mask, 0], x_train[mask, 1],\n",
        "                label=labels[c], c=colors[c], alpha=0.6)\n",
        "\n",
        "plt.xlabel(\"Sepal length (cm)\")\n",
        "plt.ylabel(\"Sepal width (cm)\")\n",
        "plt.title(\"Training Set\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45f2aba5-4729-4975-96bd-724d48885099",
      "metadata": {
        "id": "45f2aba5-4729-4975-96bd-724d48885099"
      },
      "source": [
        "## Naive Bayes Theory\n",
        "\n",
        "The Naive Bayes classifier uses Bayes' rule:\n",
        "\n",
        "$$P(Y=k | X_1, X_2) = \\frac{P(X_1, X_2 | Y=k) \\cdot P(Y=k)}{P(X_1, X_2)}$$\n",
        "\n",
        "The \"naive\" assumption is that features are independent given the class:\n",
        "\n",
        "$$P(X_1, X_2 | Y=k) = P(X_1 | Y=k) \\cdot P(X_2 | Y=k)$$\n",
        "\n",
        "For prediction, we choose the class with maximum posterior probability:\n",
        "\n",
        "$$\\hat{Y} = \\arg\\max_k P(Y=k | X_1, X_2) \\propto P(X_1 | Y=k) \\cdot P(X_2 | Y=k) \\cdot P(Y=k)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14d26dbc-156a-410f-920d-48067b561472",
      "metadata": {
        "id": "14d26dbc-156a-410f-920d-48067b561472"
      },
      "source": [
        "## Exercise 1: Compute the Class Prior\n",
        "\n",
        "The prior probability is simply the proportion of samples in each class:\n",
        "\n",
        "$$P(Y=k) = \\frac{\\text{number of samples in class } k}{\\text{total number of samples}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21be93ed-b7c8-4625-89bd-d09356077a7d",
      "metadata": {
        "id": "21be93ed-b7c8-4625-89bd-d09356077a7d"
      },
      "outputs": [],
      "source": [
        "def compute_prior(y):\n",
        "    \"\"\"\n",
        "    Compute prior probabilities P(Y=k) for each class.\n",
        "\n",
        "    Args:\n",
        "        y: array of labels, shape (n_samples,)\n",
        "\n",
        "    Returns:\n",
        "        priors: array of prior probabilities, shape (n_classes,)\n",
        "    \"\"\"\n",
        "    # Count occurrences of each class\n",
        "    # Hint: Use np.bincount() or np.unique() with return_counts=True\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fac6ed8b-9b19-4cac-801c-0287ab3ef1ef",
      "metadata": {
        "id": "fac6ed8b-9b19-4cac-801c-0287ab3ef1ef"
      },
      "outputs": [],
      "source": [
        "# Test your function\n",
        "priors = compute_prior(y_train)\n",
        "print(f\"Prior probabilities: {priors}\")\n",
        "print(f\"P(Setosa) = {priors[0]:.3f}\")\n",
        "print(f\"P(Others) = {priors[1]:.3f}\")\n",
        "\n",
        "# Visualize\n",
        "plt.bar([0, 1], priors, color=colors)\n",
        "plt.xticks([0, 1], labels.values())\n",
        "plt.ylabel(\"Prior Probability\")\n",
        "plt.title(\"Class Prior Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0331f481-1d44-4309-9de4-ed3f1f35a692",
      "metadata": {
        "id": "0331f481-1d44-4309-9de4-ed3f1f35a692"
      },
      "source": [
        "## Exercise 2: Compute Class-Conditional Parameters\n",
        "\n",
        "We assume each feature follows a Gaussian distribution for each class:\n",
        "\n",
        "$$P(X_i | Y=k) = \\mathcal{N}(\\mu_{ik}, \\sigma_{ik}^2)$$\n",
        "\n",
        "We estimate these parameters using maximum likelihood:\n",
        "\n",
        "$$\\mu_{ik} = \\frac{1}{n_k} \\sum_{j: y_j=k} x_{ij}$$\n",
        "\n",
        "$$\\sigma_{ik}^2 = \\frac{1}{n_k} \\sum_{j: y_j=k} (x_{ij} - \\mu_{ik})^2$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf06c05b-e169-49b7-8477-4261533c3edd",
      "metadata": {
        "id": "cf06c05b-e169-49b7-8477-4261533c3edd"
      },
      "outputs": [],
      "source": [
        "def compute_gaussian_params(x, y):\n",
        "    \"\"\"\n",
        "    Compute mean and std for each feature in each class.\n",
        "\n",
        "    Args:\n",
        "        x: array of features, shape (n_samples, n_features)\n",
        "        y: array of labels, shape (n_samples,)\n",
        "\n",
        "    Returns:\n",
        "        means: array of means, shape (n_classes, n_features)\n",
        "        stds: array of standard deviations, shape (n_classes, n_features)\n",
        "    \"\"\"\n",
        "    n_classes = len(np.unique(y))\n",
        "    n_features = x.shape[1]\n",
        "\n",
        "    means = np.zeros((n_classes, n_features))\n",
        "    stds = np.zeros((n_classes, n_features))\n",
        "\n",
        "    # TODO: Implement this function\n",
        "    # For each class k and feature i:\n",
        "    #   - Compute the mean of feature i for samples in class k\n",
        "    #   - Compute the std of feature i for samples in class k\n",
        "\n",
        "    return means, stds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7023b54d-2865-4191-9767-f53f1fa56be4",
      "metadata": {
        "id": "7023b54d-2865-4191-9767-f53f1fa56be4"
      },
      "outputs": [],
      "source": [
        "# Test your function\n",
        "means, stds = compute_gaussian_params(x_train, y_train)\n",
        "\n",
        "print(\"Class-conditional parameters:\")\n",
        "print(f\"\\nClass 0 (Setosa):\")\n",
        "print(f\"  Feature 0: mean={means[0,0]:.3f}, std={stds[0,0]:.3f}\")\n",
        "print(f\"  Feature 1: mean={means[0,1]:.3f}, std={stds[0,1]:.3f}\")\n",
        "print(f\"\\nClass 1 (Others):\")\n",
        "print(f\"  Feature 0: mean={means[1,0]:.3f}, std={stds[1,0]:.3f}\")\n",
        "print(f\"  Feature 1: mean={means[1,1]:.3f}, std={stds[1,1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ae3e4c4-f39d-43b3-a2b0-bd2a773afb6a",
      "metadata": {
        "id": "0ae3e4c4-f39d-43b3-a2b0-bd2a773afb6a"
      },
      "source": [
        "## Exercise 3: Implement the Prediction Function\n",
        "\n",
        "To make predictions, we compute the log posterior for each class:\n",
        "\n",
        "$$\\log P(Y=k | X) \\propto \\log P(Y=k) + \\sum_{i=1}^2 \\log P(X_i | Y=k)$$\n",
        "\n",
        "We use log probabilities for numerical stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfb498be-ec13-4ffd-8a58-423b242447a2",
      "metadata": {
        "id": "bfb498be-ec13-4ffd-8a58-423b242447a2"
      },
      "outputs": [],
      "source": [
        "def predict_naive_bayes(x, priors, means, stds):\n",
        "    \"\"\"\n",
        "    Predict class labels using Naive Bayes.\n",
        "\n",
        "    Args:\n",
        "        x: array of features, shape (n_samples, n_features)\n",
        "        priors: prior probabilities, shape (n_classes,)\n",
        "        means: class means, shape (n_classes, n_features)\n",
        "        stds: class stds, shape (n_classes, n_features)\n",
        "\n",
        "    Returns:\n",
        "        predictions: array of predicted labels, shape (n_samples,)\n",
        "    \"\"\"\n",
        "    n_samples = x.shape[0]\n",
        "    n_classes = len(priors)\n",
        "    # TODO: Implement this function\n",
        "    # For each sample:\n",
        "    #   1. Compute log P(Y=k) for each class k\n",
        "    #   2. Add log P(X_i | Y=k) for each feature i using norm.logpdf()\n",
        "    #   3. Choose the class with maximum log probability\n",
        "    #\n",
        "    # Hint: Use norm.logpdf(x[i, j], means[k, j], stds[k, j])\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64168c4e-2d90-4cbe-9485-df12d1855b2b",
      "metadata": {
        "id": "64168c4e-2d90-4cbe-9485-df12d1855b2b"
      },
      "outputs": [],
      "source": [
        "# Test your prediction function\n",
        "predictions = predict_naive_bayes(x_test, priors, means, stds)\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b1266ab-68e5-4fd6-a185-a30b0ded99ae",
      "metadata": {
        "id": "4b1266ab-68e5-4fd6-a185-a30b0ded99ae"
      },
      "source": [
        "## Visualize Decision Boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0594275-4d8f-457f-a18d-9e7033ce7d8f",
      "metadata": {
        "id": "c0594275-4d8f-457f-a18d-9e7033ce7d8f"
      },
      "outputs": [],
      "source": [
        "# Create a mesh to plot decision boundary\n",
        "x0_min, x0_max = x_test[:, 0].min() - 0.5, x_test[:, 0].max() + 0.5\n",
        "x1_min, x1_max = x_test[:, 1].min() - 0.5, x_test[:, 1].max() + 0.5\n",
        "xx0, xx1 = np.meshgrid(np.linspace(x0_min, x0_max, 200),\n",
        "                        np.linspace(x1_min, x1_max, 200))\n",
        "\n",
        "# Predict for all points in the mesh\n",
        "mesh_points = np.c_[xx0.ravel(), xx1.ravel()]\n",
        "Z = predict_naive_bayes(mesh_points, priors, means, stds)\n",
        "Z = Z.reshape(xx0.shape)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.contourf(xx0, xx1, Z, alpha=0.3, colors=colors)\n",
        "\n",
        "for c in [0, 1]:\n",
        "    mask = y_test == c\n",
        "    plt.scatter(x_test[mask, 0], x_test[mask, 1],\n",
        "                label=labels[c], c=colors[c], edgecolor='k')\n",
        "\n",
        "plt.xlabel(\"Sepal length (cm)\")\n",
        "plt.ylabel(\"Sepal width (cm)\")\n",
        "plt.title(f\"Naive Bayes Decision Boundary (Accuracy: {accuracy:.3f})\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c199b850-2069-4ca1-a07f-759affefb1f7",
      "metadata": {
        "id": "c199b850-2069-4ca1-a07f-759affefb1f7"
      },
      "source": [
        "## Compare with sklearn's Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bf3a41a-95e3-41d4-9639-d6d84badc187",
      "metadata": {
        "id": "5bf3a41a-95e3-41d4-9639-d6d84badc187"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Train sklearn's Naive Bayes\n",
        "sklearn_nb = GaussianNB()\n",
        "sklearn_nb.fit(x_train, y_train)\n",
        "sklearn_pred = sklearn_nb.predict(x_test)\n",
        "sklearn_accuracy = accuracy_score(y_test, sklearn_pred)\n",
        "\n",
        "print(f\"Your implementation accuracy: {accuracy:.4f}\")\n",
        "print(f\"Sklearn implementation accuracy: {sklearn_accuracy:.4f}\")\n",
        "print(f\"\\nDifference: {abs(accuracy - sklearn_accuracy):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c64835c4-5a4f-431f-9a56-a68b20648350",
      "metadata": {
        "id": "c64835c4-5a4f-431f-9a56-a68b20648350"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import multivariate_normal\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you already have: x_train, y_train, means, stds, labels, colors\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot training data points\n",
        "for c in [0, 1]:\n",
        "    mask = y_train == c\n",
        "    plt.scatter(x_train[mask, 0], x_train[mask, 1],\n",
        "                label=labels[c], c=colors[c], alpha=0.6, edgecolor='k')\n",
        "\n",
        "# Create mesh grid for contour plot\n",
        "x0_min, x0_max = x_train[:, 0].min() - 0.5, x_train[:, 0].max() + 0.5\n",
        "x1_min, x1_max = x_train[:, 1].min() - 0.5, x_train[:, 1].max() + 0.5\n",
        "xx0, xx1 = np.meshgrid(np.linspace(x0_min, x0_max, 200),\n",
        "                        np.linspace(x1_min, x1_max, 200))\n",
        "mesh_points = np.c_[xx0.ravel(), xx1.ravel()]\n",
        "\n",
        "# Compute predictions on the mesh\n",
        "Z_pred = predict_naive_bayes(mesh_points, priors, means, stds)\n",
        "Z_pred = Z_pred.reshape(xx0.shape)\n",
        "\n",
        "# Plot the boundary at level 0.5 (between classes 0 and 1)\n",
        "plt.contour(xx0, xx1, Z_pred, levels=[0.5], colors='black',\n",
        "            linewidths=3, linestyles='--')\n",
        "\n",
        "# Plot contours for each class\n",
        "for k in range(2):\n",
        "    # Create diagonal covariance matrix (Naive Bayes independence assumption)\n",
        "    cov = np.diag(stds[k]**2)\n",
        "\n",
        "    # Create multivariate normal distribution\n",
        "    mvn = multivariate_normal(mean=means[k], cov=cov)\n",
        "\n",
        "    # Evaluate probability density on mesh\n",
        "    Z = mvn.pdf(mesh_points).reshape(xx0.shape)\n",
        "\n",
        "    # Plot contours\n",
        "    plt.contour(xx0, xx1, Z, colors=colors[k], alpha=0.6, linewidths=2)\n",
        "\n",
        "plt.xlabel(\"Sepal length (cm)\")\n",
        "plt.ylabel(\"Sepal width (cm)\")\n",
        "plt.title(\"2D Gaussian Contours for Each Class\\n(Naive Bayes Independence Assumption)\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "ml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}